# +--------------------------------------------------------------------------+
# |  Licensed Materials - Property of IBM                                    |
# |                                                                          |
# | (C) Copyright IBM Corporation 2009-2026.                                 |
# +--------------------------------------------------------------------------+
# | This module complies with Django 1.0 and is                              |
# | Licensed under the Apache License, Version 2.0 (the "License");          |
# | you may not use this file except in compliance with the License.         |
# | You may obtain a copy of the License at                                  |
# | http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable |
# | law or agreed to in writing, software distributed under the License is   |
# | distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY |
# | KIND, either express or implied. See the License for the specific        |
# | language governing permissions and limitations under the License.        |
# +--------------------------------------------------------------------------+
# | Authors: IBM Application Development Team                                |
# +--------------------------------------------------------------------------+

import sys
from django.db.models.sql.where import OR
from jinja2.nodes import If, Or
from _operator import or_
_IS_JYTHON = sys.platform.startswith( 'java' )

import datetime
import copy
import re

from django.db.backends.ddl_references import (
    Columns, ForeignKeyName, IndexName, Statement, Table,
    )

try:
    from django.db.backends.schema import BaseDatabaseSchemaEditor
except ImportError:
    from django.db.backends.base.schema import BaseDatabaseSchemaEditor

import six
from django.db import models
from django.db.backends.utils import truncate_name, split_identifier
from django.db.models.fields.related import ManyToManyField
from django.db.models.fields.related import ManyToManyRel
from django.db.models.fields.composite import CompositePrimaryKey
from django.db.utils import ProgrammingError
from django import VERSION as djangoVersion

from django.db.models.expressions import Value
from django.db.models.expressions import CombinedExpression
from django.db.models.sql import Query
from django.db.models.functions import Now, ExtractYear
from django.db.models.functions import Coalesce
from django.db.models.functions.math import Pi
from django.db.models import Case
from django.db.models import NOT_PROVIDED
from decimal import Decimal
from django.db.models import CharField, TextField, DateField, DateTimeField, TimeField, IntegerField, FloatField, DecimalField

if not _IS_JYTHON:
    import ibm_db_dbi as Database
else:
    from com.ziclix.python.sql import zxJDBC as Database
Error = Database.Error

def _is_relevant_relation(relation, altered_field):
    """
    When altering the given field, must constraints on its model from the given
    relation be temporarily dropped?
    """
    field = relation.field
    if field.many_to_many:
        # M2M reverse field
        return False
    if altered_field.primary_key and field.to_fields == [None]:
        # Foreign key constraint on the primary key, which is being altered.
        return True
    # Is the constraint targeting the field being altered?
    return altered_field.name in field.to_fields


def _related_non_m2m_objects(old_field, new_field):
    # Filter out m2m objects from reverse relations.
    # Return (old_relation, new_relation) tuples.
    return zip(
        (obj for obj in old_field.model._meta.related_objects if _is_relevant_relation(obj, old_field)),
        (obj for obj in new_field.model._meta.related_objects if _is_relevant_relation(obj, new_field))
    )

class DB2SchemaEditor(BaseDatabaseSchemaEditor):
    psudo_column_prefix = 'psudo_'
    sql_create_table = "CREATE TABLE %(table)s (%(definition)s) ORGANIZE BY ROW"
    sql_delete_table = "DROP TABLE %(table)s"
    sql_rename_table = "RENAME TABLE %(old_table)s TO %(new_table)s"
    sql_create_column = "ALTER TABLE %(table)s ADD COLUMN %(column)s %(definition)s"
    sql_alter_column_type = "ALTER COLUMN %(column)s SET DATA TYPE %(type)s"
    sql_alter_column_type_from_int_to_auto = "ALTER COLUMN %(column)s SET GENERATED BY DEFAULT AS IDENTITY( START WITH %(max)d, INCREMENT BY 1, CACHE 10 ORDER )"
    sql_create_fk = "ALTER TABLE %(table)s ADD CONSTRAINT %(name)s FOREIGN KEY (%(column)s) REFERENCES %(to_table)s (%(to_column)s)"
    sql_delete_pk = "ALTER TABLE %(table)s DROP CONSTRAINT %(name)s"
    sql_delete_unique = "ALTER TABLE %(table)s DROP CONSTRAINT %(name)s"
    sql_drop_pk = "ALTER TABLE %(table)s DROP PRIMARY KEY"
    sql_drop_default = "ALTER TABLE %(table)s ALTER COLUMN %(column)s DROP DEFAULT"
    sql_create_fk_add = "%(pre_sql)s ADD CONSTRAINT %(name)s FOREIGN KEY (%(column)s) REFERENCES %(to_table)s (%(to_column)s)%(deferrable)s"
    sql_create_unique = (
        "ALTER TABLE %(table)s ADD CONSTRAINT %(name)s "
        "UNIQUE (%(columns)s)%(deferrable)s"
    )
    
    def quote_value(self, value):
        if isinstance(value, (datetime.date, datetime.time, datetime.datetime)):
            return "'%s'" % value
        elif isinstance(value, str):
            return "'%s'" % value.replace("\'", "\'\'").replace('%', '%%')
        elif isinstance(value, (bytes, bytearray, memoryview)):
            return "'%s'" % value.hex()
        elif isinstance(value, bool):
            return "1" if value else "0"
        else:
            return str(value)

    @property
    def sql_create_pk(self):
        self._reorg_tables()
        return "ALTER TABLE %(table)s ADD CONSTRAINT %(name)s PRIMARY KEY (%(columns)s)"

    def effective_default(self, field):
        """Return a field's effective database default value."""
        value = self._effective_default(field)
        if value is not None:
            value = field.get_db_prep_save(value, self.connection)
        return value
    
    def is_always_true(self, condition):
        from django.db.models.lookups import GreaterThan, GreaterThanOrEqual, LessThan, LessThanOrEqual, Exact
        # Very basic constant evaluation
        if isinstance(condition, GreaterThan):
            return (isinstance(condition.lhs, Value) and isinstance(condition.rhs, Value)
                    and condition.lhs.value > condition.rhs.value)
        if isinstance(condition, GreaterThanOrEqual):
            return (isinstance(condition.lhs, Value) and isinstance(condition.rhs, Value)
                    and condition.lhs.value >= condition.rhs.value)
        if isinstance(condition, LessThan):
            return (isinstance(condition.lhs, Value) and isinstance(condition.rhs, Value)
                    and condition.lhs.value < condition.rhs.value)
        if isinstance(condition, LessThanOrEqual):
            return (isinstance(condition.lhs, Value) and isinstance(condition.rhs, Value)
                    and condition.lhs.value <= condition.rhs.value)
        if isinstance(condition, Exact):
            return (isinstance(condition.lhs, Value) and isinstance(condition.rhs, Value)
                    and condition.lhs.value == condition.rhs.value)
        return False

    def _evaluate_expression_locally(self, expr):
        """Very limited local evaluator for constants + functions like Pi()."""
        from math import pi
    
        if isinstance(expr, Value):
            return expr.value
        if hasattr(expr, 'function') and expr.function.upper() == 'PI':
            return pi
        if isinstance(expr, CombinedExpression):
            lhs = self._evaluate_expression_locally(expr.lhs)
            rhs = self._evaluate_expression_locally(expr.rhs)
            if expr.connector == '+':
                return lhs + rhs
            elif expr.connector == '-':
                return lhs - rhs
            elif expr.connector == '*':
                return lhs * rhs
            elif expr.connector == '/':
                return lhs / rhs
        if isinstance(expr, Coalesce):
            # Evaluate each arg, return first non-null
            for arg in expr.source_expressions:
                val = self._evaluate_expression_locally(arg)
                if val is not None:
                    return val
            return None
        if isinstance(expr, Case):
            for when in expr.cases:
                condition = when.condition
                if self.is_always_true(condition):
                    return self._evaluate_expression_locally(when.result)
            if expr.default is not None:
                return self._evaluate_expression_locally(expr.default)
        raise ValueError("Expression contains non-constant parts")

    def db_default_sql(self, field):
        """Return the sql and params for the field's database default."""        

        db_default = field._db_default_expression
        if isinstance(db_default, Value):
            val = db_default.value
            if isinstance(val, (int, float, Decimal)):
                sql = "%s"  # No parentheses for numbers
            else:
                sql = self._column_default_sql(field)
        elif isinstance(db_default, (Now, Pi)):
            sql = "%s"        
        elif isinstance(db_default, (CombinedExpression, Coalesce, Case)):
            try:
                # Evaluate locally by walking the expression
                val = self._evaluate_expression_locally(db_default)
                return "%s" % val, []
            except Exception:
                # Fallback to database compilation if can't eval
                pass
        elif isinstance(db_default, ExtractYear) and isinstance(db_default.source_expressions[0], Now):
            # Compute current year in Python
            val = datetime.datetime.now().year
            return "%s" % val, []
        else:
            # Compile to string to inspect
            query = Query(model=field.model)
            compiler = query.get_compiler(connection=self.connection)
            default_sql, params = compiler.compile(db_default)
            compiled_str = default_sql.strip()
    
            # Allow certain built-in functions without wrapping in parens
            # These should start with the function name directly
            DB2_ALLOWED_FUNCTIONS = ("CURRENT_TIMESTAMP", "YEAR", "CURRENT_DATE", "CURRENT_TIME")
            if any(compiled_str.upper().startswith(func) for func in DB2_ALLOWED_FUNCTIONS):
                sql = "%s"
            else:
                # Unsupported as DB default -> wrap in (%s) so it's parameterized
                sql = "(%s)"
    
            if self.connection.features.requires_literal_defaults:
                compiled_str %= tuple(self.prepare_default(p) for p in params)
                params = []
    
            return sql % compiled_str, params
        
        query = Query(model=field.model)
        compiler = query.get_compiler(connection=self.connection)
        default_sql, params = compiler.compile(db_default)
        if self.connection.features.requires_literal_defaults:
            # Some databases don't support parameterized defaults (Oracle,
            # SQLite). If this is the case, the individual schema backend
            # should implement prepare_default().
            default_sql %= tuple(self.prepare_default(p) for p in params)
            params = []
        return sql % default_sql, params
    
    def _column_generated_sql(self, field):
        """Return the SQL to use in a GENERATED ALWAYS clause."""
        expression_sql, params = field.generated_sql(self.connection)        
        if self.connection.features.requires_literal_defaults:
            expression_sql = expression_sql % tuple(self.quote_value(p) for p in params)
            params = ()
        return f"GENERATED ALWAYS AS ({expression_sql})", params
    
    # return column definition DDL
    def column_sql(self, model, field, include_default=False, include_null=False, notnull=False, skip_force_default=False):
        db_parameter = field.db_parameters(connection=self.connection)
        sql = db_parameter['type']
        if sql is None:
            return None, None
      
        # Add database default.
        params = []
        if field.has_db_default():
            default_sql, default_params = self.db_default_sql(field)
            sql += f" DEFAULT {default_sql}"
            params.extend(default_params)
            include_default = False
            
        if include_default:
            if (field.default is not None) and field.has_default():
                value = field.get_default()
                internal_type = field.get_internal_type()
                if internal_type == 'BinaryField':
                    value2 = value
                if internal_type == 'DurationField' or internal_type == 'UUIDField' or isinstance(value, dict):
                    value = field.get_db_prep_save(value, self.connection)
                else:
                    value = self.prepare_default(value)

                if internal_type == 'UUIDField' or (isinstance(value, str) and not value.startswith("'") and not value.endswith("'")):
                    sql += " DEFAULT '%s'" % value
                else:
                    sql += " DEFAULT %s" % value
            elif not self.skip_default(field):
                value = self.effective_default(field)
                internal_type = field.get_internal_type()
                if value is not None and value != '':
                    if internal_type == 'TimeField' or internal_type == 'UUIDField' or internal_type == 'TextField':
                        sql += " DEFAULT '%s'" % value
                    elif(isinstance(value, memoryview)):
                        sql += " DEFAULT BX\'%s\'" % value.hex()
                    else:
                        sql += " DEFAULT %s" % value
                        
                    field.default = None
                elif value is not None and value == '' and (internal_type == 'CharField'):
                    sql += " DEFAULT ''"
                    field.default = None
                    
                # Adapter is not suppose to add DEFAULT clause. It should come from
                # Django ORMs defined in application. I am doing it here to run Django test suite.
                # This can be removed in future. 
                elif notnull and not skip_force_default:                
                    #Cannot add foreign key (with unique=True) column to existing table without default, DB2 throws error. 
                    if (internal_type == 'ForeignKey' and \
                         (field.unique or field.column.lower().endswith("_id"))):                
                        sql += " DEFAULT 0"

                    field.default = None
            else:
                field.default = None
        if field.generated:
            generated_sql, generated_params = self._column_generated_sql(field)
            params.extend(generated_params)
            sql += " %s" % generated_sql
        elif (not field.null or notnull) and ('DEFAULT' in sql or include_null):
            sql += " NOT NULL"
        if field.primary_key:
            sql += " PRIMARY KEY"
        elif field.unique:
            sql += " UNIQUE"
        tablespace = field.db_tablespace or model._meta.db_tablespace
        if tablespace and field.unique:
            sql += " %s" % self.connection.ops.tablespace_sql(tablespace, inline=True)

        return sql, params

    def table_sql(self, model):
        """Take a model and return its table definition."""
        # Add any unique_togethers (always deferred, as some fields might be
        # created afterward, like geometry fields with some backends).
        for field_names in model._meta.unique_together:
            fields = [model._meta.get_field(field) for field in field_names]
            self.deferred_sql.append(self._create_unique_sql(model, fields))
        # Create column SQL, add FK deferreds if needed.
        column_sqls = []
        params = []
        for field in model._meta.local_fields:
            # SQL.
            notnull = not field.null
            if field.primary_key or field.unique:
                notnull = True
                
            definition, extra_params = self.column_sql(model, field, include_default=False, include_null=True, notnull=notnull)
            if definition is None:
                continue
            # Check constraints can go on the column SQL here.
            db_params = field.db_parameters(connection=self.connection)
            if db_params["check"]:
                definition += " " + self.sql_check_constraint % db_params
            # Autoincrement SQL (for backends with inline variant).
            col_type_suffix = field.db_type_suffix(connection=self.connection)
            if col_type_suffix:
                definition += " %s" % col_type_suffix
            params.extend(extra_params)
            # FK.
            if field.remote_field and field.db_constraint:
                to_table = field.remote_field.model._meta.db_table
                to_column = field.remote_field.model._meta.get_field(
                    field.remote_field.field_name
                ).column
                if self.sql_create_inline_fk:
                    definition += " " + self.sql_create_inline_fk % {
                        "to_table": self.quote_name(to_table),
                        "to_column": self.quote_name(to_column),
                    }
                elif self.connection.features.supports_foreign_keys:
                    self.deferred_sql.append(
                        self._create_fk_sql(
                            model, field, "_fk_%(to_table)s_%(to_column)s"
                        )
                    )
            # Add the SQL to our big list.
            column_sqls.append(
                "%s %s"
                % (
                    self.quote_name(field.column),
                    definition,
                )
            )
            # Autoincrement SQL (for backends with post table definition
            # variant).
            if field.get_internal_type() in (
                "AutoField",
                "BigAutoField",
                "SmallAutoField",
            ):
                autoinc_sql = self.connection.ops.autoinc_sql(
                    model._meta.db_table, field.column
                )
                if autoinc_sql:
                    self.deferred_sql.extend(autoinc_sql)
        # The BaseConstraint DDL creation methods such as constraint_sql(),
        # create_sql(), and delete_sql(), were not designed in a way that
        # separate SQL from parameters which make their generated SQL unfit to
        # be used in a context where parametrization is delegated to the
        # backend.
        constraint_sqls = []
        if params:
            # If parameters are present (e.g. a DEFAULT clause on backends that
            # allow parametrization) defer constraint creation so they are not
            # mixed with SQL meant to be parametrized.
            for constraint in model._meta.constraints:
                self.deferred_sql.append(constraint.create_sql(model, self))
        else:
            constraint_sqls.extend(
                constraint.constraint_sql(model, self)
                for constraint in model._meta.constraints
            )

        pk = model._meta.pk
        if isinstance(pk, CompositePrimaryKey):
            constraint_sqls.append(self._pk_constraint_sql(pk.columns))

        sql = self.sql_create_table % {
            "table": self.quote_name(model._meta.db_table),
            "definition": ", ".join(
                str(statement)
                for statement in (*column_sqls, *constraint_sqls)
                if statement
            ),
        }
        if model._meta.db_tablespace:
            tablespace_sql = self.connection.ops.tablespace_sql(
                model._meta.db_tablespace
            )
            if tablespace_sql:
                sql += " " + tablespace_sql
        return sql, params

    def prepare_default(self, value):
        CONVERT_STR= (datetime.datetime, datetime.date, datetime.time, six.string_types)

        if callable(value):
            value = value()

        if isinstance(value, CONVERT_STR):
            value = "'%s'" % value
        elif isinstance(value, bool):
            value = '1' if value else '0'
        elif isinstance(value, bytes):
            value = "BX\'%s\'" % value.hex()
        else:
            value = str(value)
        return value
    
    def get_fk_name(self, model, field, suffix):
        def create_fk_name(*args, **kwargs):
            return self.quote_name(self._create_index_name(*args, **kwargs))

        name = ForeignKeyName(
            model._meta.db_table,
            [field.column],
            split_identifier(field.target_field.model._meta.db_table)[1],
            [field.target_field.column],
            suffix,
            create_fk_name,
        )

        return name

    def get_missing_constraints(self, model, constraints_pre, deferred_constraints):
        with self.connection.cursor() as cur:
            constraints_post = self.connection.introspection.get_constraints(cur, model._meta.db_table)
        for constr_name, constr_dict in list(constraints_pre.items()):
            if constr_name is not None and constr_name.find('"', 1, len(constr_name)-1) > 0:
                constr_name = constr_name.replace('"', '""')
            columns_list = []
            if len(constr_dict['columns']) > 0:                
                for cols in constr_dict['columns']:
                    if cols.find('"', 1, len(cols)-1) > 0:
                        columns_list.append(cols.replace('"', '""'))
                    else:
                        columns_list.append(cols)
            if constr_name not in constraints_post.keys():
                if constr_dict['check'] is True:
                    deferred_constraints['check'][constr_name] = columns_list
                elif constr_dict['foreign_key'] is not None:
                    deferred_constraints['fk'][constr_name] = constr_dict
                elif constr_dict['unique'] is True:
                    deferred_constraints['unique'][constr_name] = columns_list
                elif constr_dict['index'] is True and 'type' in constr_dict.keys():
                    deferred_constraints['index'][constr_name] = columns_list
                elif constr_dict['primary_key'] is True and 'type' not in constr_dict.keys():
                    deferred_constraints['pk'][constr_name] = columns_list

        return deferred_constraints

    def _is_referenced_by_fk_constraint(self, table_name, column_name=None, ignore_self=False):
        """
        Return whether or not the provided table name is referenced by another
        one. If `column_name` is specified, only references pointing to that
        column are considered. If `ignore_self` is True, self-referential
        constraints are ignored.
        """
        fk_constraints = {}
        with self.connection.cursor() as cursor:
            for other_table in self.connection.introspection.get_table_list(cursor):
                if ignore_self and other_table.name == table_name:
                    continue
                constraints = self.connection.introspection._get_foreign_key_constraints(cursor, other_table.name)                
                for key, constraint in constraints.items():
                    constraint_table, constraint_column = constraint['foreign_key']
                    if (constraint_table == table_name and
                            (column_name is None or constraint_column == column_name)):
                        if other_table.name not in fk_constraints:
                            fk_constraints[other_table.name] = {}
                        fk_constraints[other_table.name][key] = constraint
                        
        return fk_constraints
    
    def _find_and_drop_dependent_indexes(self, model, field):
        """
        Find indexes that depend on `field` (normal or expression-based),
        drop them and return a list of CREATE INDEX SQL strings to recreate later.
        """
        recreate_sqls = []
        db_table = model._meta.db_table.upper()
        # derive schema: try options.currentSchema, else fall back to username
        settings_opts = getattr(self.connection, "settings_dict", {}).get("OPTIONS", {}) or {}
        with self.connection.cursor() as cursor:
            schema = cursor.connection.get_current_schema().upper()        
        colname = field.column.upper()
    
        with self.connection.cursor() as cursor:
            cursor.execute(
                """
                SELECT i.INDNAME, ic.COLNAME, ic.TEXT
                FROM SYSCAT.INDEXES i
                JOIN SYSCAT.INDEXCOLUSE ic
                  ON i.INDSCHEMA = ic.INDSCHEMA
                 AND i.INDNAME = ic.INDNAME
                WHERE i.TABNAME = %s
                  AND i.TABSCHEMA = %s
                  AND (
                        ic.COLNAME = %s
                        OR (ic.TEXT IS NOT NULL AND UPPER(ic.TEXT) LIKE '%%' || %s || '%%')
                      )
                """,
                [db_table, schema, colname, colname],
            )
            rows = cursor.fetchall()
    
        if not rows:
            return recreate_sqls
    
        # Get all candidate CREATE INDEX SQL strings from your model
        model_index_sqls = [sql for sql in self._model_indexes_sql(model)]
    
        seen_indexes = set()
        for indname, ic_col, ic_text in rows:
            indname = indname.strip()
            if indname in seen_indexes:
                continue
            seen_indexes.add(indname)
    
            # Try to find the CREATE INDEX SQL from Django's generator
            matched = False
            for stmt in model_index_sqls:
                # Ensure we always work with raw SQL
                sql = str(stmt)
                if indname.upper() in sql.upper():
                    recreate_sqls.append(stmt)
                    matched = True
                    break
    
            # If not matched, create a best-effort recreate SQL from catalog info.
            # This is a fallback — preferable to log/raise in production if you can't
            # reconstruct exactly.
            if not matched:
                # Simple fallback: try CREATE INDEX with same name and column/expression text
                expr = ic_text or ic_col
                # expr may be 'K00' for expression index — if ic_text is present, use that
                if ic_text:
                    create_sql = f'CREATE INDEX {self.quote_name(indname)} ON {self.quote_name(db_table)} ({ic_text})'
                else:
                    create_sql = f'CREATE INDEX {self.quote_name(indname)} ON {self.quote_name(db_table)} ({self.quote_name(ic_col)})'
                recreate_sqls.append(create_sql)
    
            # Drop the index (schema-qualified)
            drop_sql = f"DROP INDEX {self.quote_name(schema)}.{self.quote_name(indname)}"
            try:
                self.execute(drop_sql)
            except Exception:
                # Some environments permit unqualified index names; try without schema
                self.execute(f"DROP INDEX {self.quote_name(indname)}")
    
        return recreate_sqls
    
    def is_column_in_generated(self, table_name, column_name):
        """
        Returns True if the given column_name is referenced by any generated column
        in the specified table, otherwise False.
    
        Args:
            table_name (str): Table name (e.g. 'TEST_IGFCOR_PONY')
            column_name (str): Column to check (e.g. 'PINK')
    
        Returns:
            bool
        """
        row = ()
        with self.connection.cursor() as cursor:
            table_schema = cursor.connection.get_current_schema().upper() 
              
            sql = """
                SELECT COLNAME
                FROM SYSCAT.COLUMNS
                WHERE TABSCHEMA = '%(schema)s'
                  AND TABNAME = '%(table)s'
                  AND GENERATED IN ('A', 'B')  -- A=ALWAYS, B=BY DEFAULT
                  AND TEXT LIKE '%%%(column)s%%'
                FETCH FIRST 1 ROWS ONLY
            """
            sql = sql % {'schema': table_schema.upper(), 'table': table_name.upper(), 'column': column_name.upper()}
            cursor.execute(sql)
            row = cursor.fetchone()
            cursor.close()
            
        return row

    def alter_field(self, model, old_field, new_field, strict=False):
                
        """
        Allow a field's type, uniqueness, nullability, default, column,
        constraints, etc. to be modified.
        `old_field` is required to compute the necessary changes.
        If `strict` is True, raise errors if the old column does not match
        `old_field` precisely.
        """
        if not self._field_should_be_altered(old_field, new_field):
            return
        # Ensure this field is even column-based
        old_db_params = old_field.db_parameters(connection=self.connection)
        old_type = old_db_params['type']
        new_db_params = new_field.db_parameters(connection=self.connection)
        new_type = new_db_params['type']
        modifying_generated_field = False
        if ((old_type is None and old_field.remote_field is None) or
                (new_type is None and new_field.remote_field is None)):
            raise ValueError(
                "Cannot alter field %s into %s - they do not properly define "
                "db_type (are you using a badly-written custom field?)" %
                (old_field, new_field),
            )
        elif old_type is None and new_type is None and (
                old_field.remote_field.through and new_field.remote_field.through and
                old_field.remote_field.through._meta.auto_created and
                new_field.remote_field.through._meta.auto_created):
            return self._alter_many_to_many(model, old_field, new_field, strict)
        elif old_type is None and new_type is None and (
                old_field.remote_field.through and new_field.remote_field.through and
                not old_field.remote_field.through._meta.auto_created and
                not new_field.remote_field.through._meta.auto_created):
            # Both sides have through models; this is a no-op.
            return
        elif old_type is None or new_type is None:
            raise ValueError(
                "Cannot alter field %s into %s - they are not compatible types "
                "(you cannot alter to or from M2M fields, or add or remove "
                "through= on M2M fields)" % (old_field, new_field)
            )
        elif (old_field.column != new_field.column):
            column = self.is_column_in_generated(model._meta.db_table, old_field.column)
            if column:
                nf = '%s.%s' % (model._meta.label, column[0].lower()) 
                raise ValueError(
                    f"Modifying GeneratedFields is not supported - the field {nf} "
                    "must be removed and re-added with the new definition."
                )
        elif old_field.generated != new_field.generated or (
            new_field.generated and old_field.db_persist != new_field.db_persist
        ):
            modifying_generated_field = True
        elif new_field.generated:
            try:
                old_field_sql = old_field.generated_sql(self.connection)
            except FieldError:
                # Field used in a generated field was renamed.
                modifying_generated_field = True
            else:
                new_field_sql = new_field.generated_sql(self.connection)
                modifying_generated_field = old_field_sql != new_field_sql
        if modifying_generated_field:
            raise ValueError(
                f"Modifying GeneratedFields is not supported - the field {new_field} "
                "must be removed and re-added with the new definition."
            )

        self._alter_field(model, old_field, new_field, old_type, new_type,
                          old_db_params, new_db_params, strict)
    
    def _field_should_be_altered(self, old_field, new_field, ignore=None):
        if not old_field.concrete and not new_field.concrete:
            return False
        ignore = ignore or set()
        _, old_path, old_args, old_kwargs = old_field.deconstruct()
        _, new_path, new_args, new_kwargs = new_field.deconstruct()
        # Don't alter when:
        # - changing only a field name
        # - changing an attribute that doesn't affect the schema
        # - changing an attribute in the provided set of ignored attributes
        # - adding only a db_column and the column name is not changed
        # - db_table does not change for model referenced by foreign keys
        for attr in ignore.union(old_field.non_db_attrs):
            old_kwargs.pop(attr, None)
        for attr in ignore.union(new_field.non_db_attrs):
            new_kwargs.pop(attr, None)
        if (
            not new_field.many_to_many
            and old_field.remote_field
            and new_field.remote_field
            and old_field.remote_field.model._meta.db_table
            == new_field.remote_field.model._meta.db_table
        ):
            old_kwargs.pop("to", None)
            new_kwargs.pop("to", None)
        # db_default can take many form but result in the same SQL.
        if (
            old_kwargs.get("db_default")
            and new_kwargs.get("db_default")
            and self.db_default_sql(old_field) == self.db_default_sql(new_field)
        ):
            old_kwargs.pop("db_default")
            new_kwargs.pop("db_default")
        
        for kwargs in (old_kwargs, new_kwargs):
            if ("default" in kwargs and kwargs["default"] in [None, NOT_PROVIDED]):
                del kwargs["default"]
        
        return self.quote_name(old_field.column) != self.quote_name(
            new_field.column
        ) or (old_path, old_args, old_kwargs) != (new_path, new_args, new_kwargs)
        
    def _alter_field(self, model, old_field, new_field, old_type, new_type,
                     old_db_params, new_db_params, strict=False):
        
        def create_fk_name(*args, **kwargs):
            return self.quote_name(self._create_index_name(*args, **kwargs))
        
        """Perform a "physical" (non-ManyToMany) field update."""
        deferred_constraints = {
            'pk': {},
            'fk': {},
            'unique': {},
            'index': {},
            'check': {}}
        
        # Drop any FK constraints, we'll remake them later
        fks_dropped = set()
        if (
            self.connection.features.supports_foreign_keys and
            old_field.remote_field and
            old_field.db_constraint and
            self._field_should_be_altered(
                old_field,
                new_field,
                ignore={"db_comment",
                        "blank",
                        "editable",
                        "error_messages",
                        "help_text",
                        "limit_choices_to",
                        "related_name",
                        "related_query_name",
                        "validators",
                        "verbose_name",},
            )
        ):        
            fk_names = self._constraint_names(model, [old_field.column], foreign_key=True)
            if strict and len(fk_names) != 1:
                raise ValueError("Found wrong number (%s) of foreign key constraints for %s.%s" % (
                    len(fk_names),
                    model._meta.db_table,
                    old_field.column,
                ))
            for fk_name in fk_names:
                fks_dropped.add((old_field.column,))
                self.execute(self._delete_fk_sql(model, fk_name))
        # Has unique been removed?
        if old_field.unique and (not new_field.unique or self._field_became_primary_key(old_field, new_field)):
            # Find the unique constraint for this field
            meta_constraint_names = {constraint.name for constraint in model._meta.constraints}
            constraint_names = self._constraint_names(
                model, [old_field.column], unique=True, primary_key=False,
                exclude=meta_constraint_names,
            )
            if strict and len(constraint_names) != 1:
                raise ValueError("Found wrong number (%s) of unique constraints for %s.%s" % (
                    len(constraint_names),
                    model._meta.db_table,
                    old_field.column,
                ))
            for constraint_name in constraint_names:
                self.execute(self._delete_unique_sql(model, constraint_name))
        # Drop incoming FK constraints if the field is a primary key or unique,
        # which might be a to_field target, and things are going to change.
        drop_foreign_keys = (
            self.connection.features.supports_foreign_keys and (
                (old_field.primary_key and new_field.primary_key) or
                (old_field.unique and new_field.unique)
            ) and old_type != new_type
        )

        related_non_m2m_objects = []
        if drop_foreign_keys:
            # '_meta.related_field' also contains M2M reverse fields, these
            # will be filtered out
            related_non_m2m_objects = list(_related_non_m2m_objects(old_field, new_field))
            for _old_rel, new_rel in related_non_m2m_objects:
                rel_fk_names = self._constraint_names(
                    new_rel.related_model, [new_rel.field.column], foreign_key=True
                )
                for fk_name in rel_fk_names:
                    self.execute(self._delete_fk_sql(new_rel.related_model, fk_name))

        # Removed an index? (no strict check, as multiple indexes are possible)
        # Remove indexes if db_index switched to False or a unique constraint
        # will now be used in lieu of an index. The following lines from the
        # truth table show all True cases; the rest are False:
        #
        # old_field.db_index | old_field.unique | new_field.db_index | new_field.unique
        # ------------------------------------------------------------------------------
        # True               | False            | False              | False
        # True               | False            | False              | True
        # True               | False            | True               | True
        if old_field.db_index and not old_field.unique and (not new_field.db_index or new_field.unique):
            # Find the index for this field
            meta_index_names = {index.name for index in model._meta.indexes}
            # Retrieve only BTREE indexes since this is what's created with
            # db_index=True.
            index_names = self._constraint_names(
                model, [old_field.column], index=True, type_=models.Index.suffix,
                exclude=meta_index_names,
            )
            for index_name in index_names:
                # The only way to check if an index was created with
                # db_index=True or with Index(['field'], name='foo')
                # is to look at its name (refs #28053).
                self.execute(self._delete_index_sql(model, index_name))
        # Change check constraints?
        meta_constraints_check = {}
        old_db_check = self._field_db_check(old_field, old_db_params)
        new_db_check = self._field_db_check(new_field, new_db_params)
        if old_db_check != new_db_check and old_db_check:        
            meta_constraint_names = {constraint.name for constraint in model._meta.constraints}
            constraint_names = self._constraint_names(
                model, [old_field.column], check=True,
                exclude=meta_constraint_names,
            )            
            
            if strict and len(constraint_names) != 1:
                raise ValueError("Found wrong number (%s) of check constraints for %s.%s" % (
                    len(constraint_names),
                    model._meta.db_table,
                    old_field.column,
                ))
            for constraint_name in constraint_names:
                self.execute(self._delete_check_sql(model, constraint_name))

            #Have only meta constraints to recreate later
            with self.connection.cursor() as cur:
                meta_constraints_check = self.connection.introspection.get_constraints(cur, model._meta.db_table)
                for key, constraint in list(meta_constraints_check.items()):
                    if key not in meta_constraint_names:
                        del meta_constraints_check[key]
                
        # Next, start accumulating actions to do
        actions = []
        null_actions = []
        old_type_org = old_type
        new_type_org = new_type
        if old_type != new_type or old_field.get_internal_type() != new_field.get_internal_type():
            old_field, new_field = self.alterFieldDataTypeByRemaking(model, old_field, new_field, strict)
            old_db_params = old_field.db_parameters(connection=self.connection)
            old_type = old_db_params['type']
            new_db_params = new_field.db_parameters(connection=self.connection)
            new_type = new_db_params['type']
            
            #Now change datatype of foreign key tables.
            for _old_rel, new_rel in related_non_m2m_objects:
                sql = self.sql_alter_column_type % {
                    'column': self.quote_name(new_rel.remote_field.column),
                    'type': new_type
                }
                self.execute(
                        self.sql_alter_column % {
                            'table': self.quote_name(new_rel.remote_field.model._meta.db_table),
                            'changes': sql
                        }
                    )
        
        needs_remake = ((old_field.primary_key or (not new_field.null and not new_field.has_default()))
                        and not (old_field.name == new_field.name and 
                                 (new_field.get_internal_type() == 'OneToOneField' or
                                  new_field.get_internal_type() == 'ForeignKey'))
                        #and not (not new_field.primary_key and new_field.unique)
                        )
        #Adding comment is already handled in alterFieldDataTypeByRemaking() under add_field_pre() when non-primary columns are modified.
        #Hence skip altering comment here.
        if not ((old_type_org != new_type_org or old_field.get_internal_type() != new_field.get_internal_type()) and not needs_remake):
            # Add field comment, if required.
            if (
                new_field.db_comment or (old_field.db_comment != new_field.db_comment)
                and self.connection.features.supports_comments
                and not self.connection.features.supports_comments_inline
            ):
                db_params = new_field.db_parameters(connection=self.connection)
                field_type = db_params["type"]
                self.execute(
                    *self._alter_column_comment_sql(
                        model, new_field, field_type, new_field.db_comment
                    )
                )
            
        # Renaming primary key column with data type change is already handled in alterFieldDataTypeByRemaking() above.
        # Hence skip renaming again.
        if (not ((old_type_org != new_type_org or old_field.get_internal_type() != new_field.get_internal_type()) and needs_remake)
            # Have they renamed the column?
            and (old_field.column != new_field.column)):
            #Need to change the field name
            
            # Drop incoming FK constraints for non-primary key fields 
            fk_constraints = self._is_referenced_by_fk_constraint(model._meta.db_table, old_field.column, ignore_self=True)
            for table, contraints in fk_constraints.items():
                for constr_name in contraints.keys():
                    self.execute(self.sql_delete_pk % {
                        'table': self.quote_name(table),
                        'name': self.quote_name(constr_name)})
                    
            #Defer constraint check
            with self.connection.cursor() as cur:
                constraints_pre = self.connection.introspection.get_constraints(cur, model._meta.db_table)                
                self._defer_constraints_check(constraints_pre, deferred_constraints, old_field, new_field, model, defer_pk=True, defer_unique=True, defer_index=True, defer_check=True)

            self.execute(
                self.sql_rename_column % {
                    'table': self.quote_name(model._meta.db_table),
                    'old_column': self.quote_name(old_field.column),
                    'new_column': self.quote_name(new_field.column),
                }
            )
            
            for i, sql in enumerate(self.deferred_sql):
                if isinstance(sql, Statement):
                    sql.rename_column_references(model._meta.db_table, old_field.column, new_field.column)
                elif isinstance(sql, str):
                    self.deferred_sql[i] = sql.replace(old_field.column.upper(), new_field.column.upper())

            #restore meta constraint checks which got deleted while renaming or altering column
            def _get_meta_constraint_check(model_meta_constraints, constraint_name):
                for constraint in model_meta_constraints:
                    if constraint.name == constraint_name:
                        return constraint
                
            for constraint_name, value in meta_constraints_check.items():
                if constraint_name not in constraints_pre.keys():
                    self.add_constraint(model, _get_meta_constraint_check(model._meta.constraints, constraint_name))                    
                    
            deferred_constraints = self.get_missing_constraints(model, constraints_pre, deferred_constraints)
            self._restore_constraints_check(deferred_constraints, old_field, new_field, model)
            
            #ALTER TABLE "SCHEMA_BOOK" ADD CONSTRAINT "SCHEMA_BOOK_AUTHOR_ID_C80C8297_FK_SCHEMA_AUTHOR_NAME" FOREIGN KEY ("AUTHOR_ID") REFERENCES "SCHEMA_AUTHOR" ("NAME"); (params ())
            for table, contraints in fk_constraints.items():
                for constr_name, constraint in contraints.items():
                    suffix = '_fk_%(to_table)s_%(to_column)s'
                    to_table = constraint['foreign_key'][0].upper()
                    to_column = new_field.column.upper()
                    column = ', '.join(column for column in constraint['columns']).upper()
                    fk_name = ForeignKeyName(table.upper(),
                        [column],
                        to_table,
                        [to_column],
                        suffix,
                        create_fk_name,
                    )
                                                                                
                    self.execute(
                        self.sql_create_fk % {
                            'table': self.quote_name(table),                            
                            'name': fk_name,
                            'column': column,
                            'to_table': to_table,
                            'to_column': to_column,
                        }
                    )            

        if new_field.has_db_default():
            if (
                not old_field.has_db_default()
                or new_field.db_default != old_field.db_default
            ):
                actions.append(
                    self._alter_column_database_default_sql(model, old_field, new_field)
                )
        elif old_field.has_db_default():
            actions.append(
                self._alter_column_database_default_sql(
                    model, old_field, new_field, drop=True
                )
            )
            
        # When changing a column NULL constraint to NOT NULL with a given
        # default value, we need to perform 4 steps:
        #  1. Add a default for new incoming writes
        #  2. Update existing NULL rows with new default
        #  3. Replace NULL constraint with NOT NULL
        #  4. Drop the default again.
        # Default change?
        old_default = self.effective_default(old_field)
        new_default = self.effective_default(new_field)
        needs_database_default = (
            old_field.null and
            not new_field.null and
            not new_field.has_db_default() and
            not self.skip_default(new_field) and
            old_default != new_default and
            new_default is not None
        )
        if needs_database_default:
            actions.append(self._alter_column_default_sql(model, old_field, new_field))
            
        # Nullability change?
        recreate_sqls = []
        if old_field.null != new_field.null:
            recreate_sqls = self._find_and_drop_dependent_indexes(model, old_field)
            fragment = self._alter_column_null_sql(model, old_field, new_field)
            if fragment:
                null_actions.append(fragment)

        # Only if we have a default and there is a change from NULL to NOT NULL
        four_way_default_alteration = (
            new_field.has_default() or new_field.has_db_default()
            ) and (old_field.null and not new_field.null)

        if actions or null_actions:
            if not four_way_default_alteration:
                # If we don't have to do a 4-way default alteration we can
                # directly run a (NOT) NULL alteration
                actions = actions + null_actions
            # Combine actions together if we can (e.g. postgres)
            if self.connection.features.supports_combined_alters and actions:
                sql, params = tuple(zip(*actions))
                actions = [(", ".join(sql), sum(params, []))]
            # Apply those actions
            for sql, params in actions:
                self.execute(
                    self.sql_alter_column % {
                        "table": self.quote_name(model._meta.db_table),
                        "changes": sql,
                    },
                    params,
                )
            if four_way_default_alteration:
                if not new_field.has_db_default():
                    default_sql = "%s"
                    params = [new_default]
                else:
                    default_sql, params = self.db_default_sql(new_field)
                # Update existing rows with default value
                self.execute(
                    self.sql_update_with_default % {
                        "table": self.quote_name(model._meta.db_table),
                        "column": self.quote_name(new_field.column),
                        "default": default_sql,
                    },
                    params,
                )
                # Since we didn't run a NOT NULL change before we need to do it
                # now
                for sql, params in null_actions:
                    self.execute(
                        self.sql_alter_column % {
                            "table": self.quote_name(model._meta.db_table),
                            "changes": sql,
                        },
                        params,
                    )        
        
        # If primary_key changed to False, delete the primary key constraint.
        if old_field.primary_key and not new_field.primary_key:
            self._delete_primary_key(model, strict)
        # Added a unique?
        if self._unique_should_be_added(old_field, new_field):
            self.execute(self._create_unique_sql(model, [new_field]))
        # Added an index? Add an index if db_index switched to True or a unique
        # constraint will no longer be used in lieu of an index. The following
        # lines from the truth table show all True cases; the rest are False:
        #
        # old_field.db_index | old_field.unique | new_field.db_index | new_field.unique
        # ------------------------------------------------------------------------------
        # False              | False            | True               | False
        # False              | True             | True               | False
        # True               | True             | True               | False
        if (not old_field.db_index or old_field.unique) and new_field.db_index and not new_field.unique:
            self.execute(self._create_index_sql(model, fields=[new_field]))
        # Type alteration on primary key? Then we need to alter the column
        # referring to us.
        rels_to_update = []
        if drop_foreign_keys:
            rels_to_update.extend(_related_non_m2m_objects(old_field, new_field))
        # Changed to become primary key?
        if self._field_became_primary_key(old_field, new_field):
            # Make the new one
            self.execute(self._create_primary_key_sql(model, new_field))
            # Update all referencing columns
            rels_to_update.extend(_related_non_m2m_objects(old_field, new_field))
        # Handle our type alters on the other end of rels from the PK stuff above
        for old_rel, new_rel in rels_to_update:
            rel_db_params = new_rel.field.db_parameters(connection=self.connection)
            rel_type = rel_db_params['type']
            fragment, other_actions = self._alter_column_type_sql(
                new_rel.related_model, old_rel.field, new_rel.field, rel_type, None, None
            )
            self.execute(
                self.sql_alter_column % {
                    "table": self.quote_name(new_rel.related_model._meta.db_table),
                    "changes": fragment[0],
                },
                fragment[1],
            )
            for sql, params in other_actions:
                self.execute(sql, params)
        # Does it have a foreign key?
        if (self.connection.features.supports_foreign_keys and new_field.remote_field and
                (fks_dropped or not old_field.remote_field or not old_field.db_constraint) and
                new_field.db_constraint):
            name = str(self.get_fk_name(model, new_field, "_fk_%(to_table)s_%(to_column)s")).strip('\"')
            if not any(True for sql in self.deferred_sql if name in str(sql).strip('\"')):
                self.execute(self._create_fk_sql(model, new_field, "_fk_%(to_table)s_%(to_column)s"))
        # Rebuild FKs that pointed to us if we previously had to drop them
        if drop_foreign_keys:
            for rel in new_field.model._meta.related_objects:
                if _is_relevant_relation(rel, new_field) and rel.field.db_constraint:
                    self.deferred_sql.append(self._create_fk_sql(rel.related_model, rel.field, "_fk"))

        if new_field.remote_field and new_field.db_constraint:
            name = str(self.get_fk_name(model, new_field, "_fk_%(to_table)s_%(to_column)s")).strip('\"')
            with self.connection.cursor() as cur:
                constraints = self.connection.introspection.get_constraints(cur, model._meta.db_table)
            if (name.lower() not in constraints.keys()) and not any(True for sql in self.deferred_sql if name in str(sql).strip('\"')):
                self.execute(self._create_fk_sql(model, new_field, "_fk_%(to_table)s_%(to_column)s"))
                if old_field.unique != new_field.unique and old_field.unique or \
                    ( old_field.unique and old_field.primary_key != new_field.primary_key and
                      not old_field.primary_key):
                    field_index = self._field_indexes_sql(model, new_field)
                    if len(field_index) > 0:
                        self.execute(field_index[0])

        # Does it have check constraints we need to add?
        if old_db_params['check'] != new_db_params['check'] and new_db_params['check']:
            # Check if a custom CheckConstraint exists for this field
            custom_constraint = None
            if hasattr(model._meta, 'constraints') and model._meta.constraints:
                from django.db.models import CheckConstraint
                for constraint in model._meta.constraints:
                    if isinstance(constraint, CheckConstraint):
                        constraint_field_names = self._extract_field_names_from_constraint(constraint)
                        if old_field.name in constraint_field_names:
                            custom_constraint = constraint
                            break
                        
            if custom_constraint:
                constraint_name = custom_constraint.name
                self.execute(self._create_check_sql(model, constraint_name, new_db_params['check']))
                
            constraint_name = self._create_index_name(model._meta.db_table, [new_field.column], suffix='_check')
            self.execute(self._create_check_sql(model, constraint_name, new_db_params['check']))
        # Drop the default if we need to
        # (Django usually does not use in-database defaults)
        if needs_database_default:
            changes_sql, params = self._alter_column_default_sql(model, old_field, new_field, drop=True)
            sql = self.sql_alter_column % {
                "table": self.quote_name(model._meta.db_table),
                "changes": changes_sql,
            }
            self.execute(sql, params)
            
        #recreate indexes if dropped for NULLability change
        for sql in recreate_sqls:
            if sql:
                self.execute(sql)
                
        # Reset connection if required
        if self.connection.features.connection_persists_old_columns:
            self.connection.close()

    def add_field_pre(self, model, field, unique, notnull=False, include_null=False):
        """
        Create a field on a model. Usually involves adding a column, but may
        involve adding a table instead (for M2M fields).
        """
        # Special-case implicit M2M tables
        if field.many_to_many and field.remote_field.through._meta.auto_created:
            return self.create_model(field.remote_field.through), True
        # Get the column's definition
        definition, params = self.column_sql(model, field, include_default=True, include_null=include_null, notnull=notnull)        
        # It might not actually have a column behind it
        if definition is None:
            return None, True
    
        # Below fetches suffix for AutoField's (Generated By clause)
        if col_type_suffix := field.db_type_suffix(connection=self.connection):
            definition += f" {col_type_suffix}"
            
        # Check constraints can go on the column SQL here
        db_params = field.db_parameters(connection=self.connection)
        if db_params['check']:
            definition += " CHECK (%s)" % db_params['check']

        # Build the SQL and run it
        sql = self.sql_create_column % {
            "table": self.quote_name(model._meta.db_table),
            "column": self.quote_name(field.column),
            "definition": definition,
        }
        
        field_default_set = False
        if sql and "DEFAULT" in sql.upper():
            field_default_set = True

        switch_on_integrity = 0
        if sql and "GENERATED ALWAYS AS" in sql.upper():
            sqli = 'SET INTEGRITY FOR %s OFF' % model._meta.db_table
            self.execute(sqli)
            switch_on_integrity = 1

        self.execute(sql, params)

        if switch_on_integrity:
            sqli = 'SET INTEGRITY FOR %s IMMEDIATE CHECKED FORCE GENERATED' % model._meta.db_table
            self.execute(sqli)

        # Drop the default if we need to and only if its set before
        # (Django usually does not use in-database defaults)
        if field_default_set:
            if not field.has_db_default() and not self.skip_default(field) and self.effective_default(field) is not None:
                changes_sql, params = self._alter_column_default_sql(model, None, field, drop=True)
                sql = self.sql_alter_column % {
                    "table": self.quote_name(model._meta.db_table),
                    "changes": changes_sql,
                }
                self.execute(sql, params)
                
                #If default is dropped no need of NOT NULL
                changes_sql = self.sql_alter_column_null % {
                    "column": self.quote_name(field.column),
                }
                sql = self.sql_alter_column % {
                    "table": self.quote_name(model._meta.db_table),
                    "changes": changes_sql,
                }
                self.execute(sql)
        
        # Add field comment, if required.
        if (
            field.db_comment
            and self.connection.features.supports_comments
            and not self.connection.features.supports_comments_inline
        ):
            field_type = db_params["type"]
            self.execute(
                *self._alter_column_comment_sql(
                    model, field, field_type, field.db_comment
                )
            )
            
        field.unique = unique
        # Add an index, if required
        self.deferred_sql.extend(self._field_indexes_sql(model, field))
        # Add any FK constraints later
        
        if field.remote_field and self.connection.features.supports_foreign_keys and field.db_constraint:
            self.deferred_sql.append(self._create_fk_sql(model, field, "_fk_%(to_table)s_%(to_column)s"))
        # Reset connection if required
        if self.connection.features.connection_persists_old_columns:
            self.connection.close()

        return None, False

    def add_field_post(self, model, field, p_key, unique, notnull = False):
        if( djangoVersion[0:2] < ( 1, 9 ) ):
            if field.rel is not None and hasattr(field.rel,'through'):
                rel_condition = field.rel.through._meta.auto_created
            else:
                rel_condition = False
        else:
            if field.remote_field is not None and hasattr(field.remote_field,'through'):
                #rel_condition = field.remote_field.through._meta.auto_created
                rel_condition = True
            else:
                rel_condition = False
                
        if isinstance(field, ManyToManyField) and rel_condition:
            return
        else:
            self._reorg_tables()
        sql = None

        # Autoincrement SQL (for backends with inline variant)
        col_type_suffix = field.db_type_suffix(connection=self.connection)

        if notnull or unique or p_key or col_type_suffix:
            del_column = self.sql_delete_column % {'table': self.quote_name(model._meta.db_table), 'column': self.quote_name(field.column)}
            if notnull:
                field.null = False
                sql = self.sql_alter_column_not_null % {'column': self.quote_name(field.column)}
                sql = self.sql_alter_column % {'table': self.quote_name(model._meta.db_table), 'changes': sql}
                try:
                    self.execute(sql)
                    self._reorg_tables()
                except Error as e:
                    self.execute(del_column)
                    raise e
            if p_key:
                field.primary_key = True
                cur = self.connection.cursor()
                #remove other pk if available
                table = model._meta.db_table.replace('""', '\"') if model._meta.db_table.count("\"") > 0 else model._meta.db_table
                other_pk = None
                for other_pk in cur.connection.primary_keys( True, cur.connection.get_current_schema(), table):
                    self.execute(
                        self.sql_drop_pk % {                        
                            'table': self.quote_name(model._meta.db_table),                            
                        }
                    )
                    
                if other_pk is None:
                    cur = self.connection.cursor()
                    table = self.quote_name(table)
                    sql = "SELECT NAME FROM SYSIBM.SYSCOLUMNS WHERE TBNAME = %s AND KEYSEQ > 0 ORDER BY KEYSEQ ASC" % table.replace("\"","\'")
                    cur.execute(sql)
                    other_pk = cur.fetchone()
                    if other_pk:
                        self.execute(
                            self.sql_drop_pk % {                        
                                'table': self.quote_name(model._meta.db_table),                            
                            }
                        )
                    
                    #Note: Need to check if we need to break from loop here after first execution.
                sql = self.sql_create_pk % {'table': self.quote_name(model._meta.db_table), 'name': self.quote_name(self._create_index_name(model._meta.db_table, [field.column], suffix="_pk")), 'columns': self.quote_name(field.column)}
                self.deferred_sql.append(sql)
            elif unique:
                constraint_name = self._create_index_name(model._meta.db_table, [field.column], suffix="_uniq")
                sql = self.sql_create_unique % {'table': self.quote_name(model._meta.db_table), 'name': constraint_name, 'columns': self.quote_name(field.column), 'deferrable': self._deferrable_constraint_sql(None)}
                try:
                    self.execute(sql)
                    self._reorg_tables()
                except Error as e:
                    self.execute(del_column)
                    raise e
            if col_type_suffix:
                cur = self.connection.cursor()
                try:
                    table = model._meta.db_table.replace('""', '\"') if model._meta.db_table.count("\"") > 0 else model._meta.db_table
                    table = self.quote_name(table)                    
                    sql = "SELECT NAME, IDENTITY FROM SYSIBM.SYSCOLUMNS WHERE TBNAME=%(table)s and IDENTITY='Y'" % {'table': table.replace("\"","\'")}
                    cur.execute(sql)
                    identity = cur.fetchone()
                    
                    if identity:
                        idy = identity[0]
                        if idy.count("\"") > 0:
                            idy = idy.replace('"', '""')
                        sql = "ALTER TABLE %(table)s ALTER COLUMN %(id)s DROP IDENTITY" % {'table': self.quote_name(model._meta.db_table), 'id':self.quote_name(idy)}
                        cur.execute(sql)
                    sql = "SELECT MAX(%(column)s) FROM %(table)s" % {'column': self.quote_name(field.column), 'table': self.quote_name(model._meta.db_table)}
                    cur.execute(sql)
                    maxVal = cur.fetchone()
                    sql = "ALTER TABLE %(table)s ALTER COLUMN %(column)s SET GENERATED BY DEFAULT AS IDENTITY (START WITH %(maxVal)s, INCREMENT BY 1, CACHE 10 ORDER)" % {
                        'table': self.quote_name(model._meta.db_table),
                        'column': self.quote_name(field.column),
                        'maxVal': 1 if maxVal is None or maxVal[0] is None else maxVal[0]+1}
                    self.deferred_sql.append(sql)
                except Error as e:
                    self.execute(del_column)
                    raise e
    
    def getM2MTablesConstraints(self, model):
        """
        Get all FK, Unique, Index constraints for M2M tables related to the given model.
        This includes both forward and reverse M2M relationships. and
        any FK columns that transitively reference model's PK via MTI.
        """
        m2m_constraints = {}
    
        for field in model._meta.get_fields():
            if field.many_to_many and hasattr(field, 'through'):
                through = field.through
                if not through._meta.auto_created:
                    continue  # Skip manually defined through tables
    
                m2m_table = through._meta.db_table
    
                for through_field in through._meta.local_fields:
                    if through_field.is_relation and through_field.column:
                        related_model = getattr(through_field.remote_field, 'model', None)
                        # Include constraints if the FK points to the current model
                        if related_model == model:
                            m2m_fk_column = through_field.column
                            with self.connection.cursor() as cursor:
                                m2m_constraints[(m2m_table, m2m_fk_column)] = self.connection.introspection.get_constraints(cursor, m2m_table)
    
        # Additionally, check all models in the project to find reverse M2M relations
        for related in model._meta.apps.get_models():
            for field in related._meta.get_fields():
                if field.many_to_many and hasattr(field, 'through'):
                    through = field.through
                    if not through._meta.auto_created:
                        continue
    
                    m2m_table = through._meta.db_table
    
                    for through_field in through._meta.local_fields:
                        if through_field.is_relation and through_field.column:
                            related_model = getattr(through_field.remote_field, 'model', None)
                            if related_model == model:
                                m2m_fk_column = through_field.column
                                with self.connection.cursor() as cursor:
                                    m2m_constraints[(m2m_table, m2m_fk_column)] = self.connection.introspection.get_constraints(cursor, m2m_table)
                        
        return m2m_constraints

    def getAllReferencingFKs(self, model):
        """
        For the given model (whose PK type is changing), find all (table, column) pairs
        where a ForeignKey (direct or via MTI chain) ultimately points to this model's PK.
        Returns a list of tuples: (referencing_table, referencing_column)
        """
        referencing = []
        all_models = list(model._meta.apps.get_models())
        for other_model in all_models:
            for field in other_model._meta.local_fields:
                if getattr(field, "is_relation", False) and hasattr(field, "remote_field") and field.remote_field:
                    fk_model = field.remote_field.model
                    current = fk_model
                    while current is not None:
                        if current == model:
                            referencing.append((other_model._meta.db_table, field.column))
                            break
                        # Go up MTI parent if present
                        parents = list(getattr(current._meta, "get_parent_list", lambda: [])())
                        current = parents[0] if parents else None
        return referencing

    def alterFieldDataTypeForM2MTables_post(self, m2m_table, m2m_fk_column, constraints, pk_new_type, drop_fk=False):
        # --- Drop constraints ---                                
        for cname, cdata in constraints.items():                
            if m2m_fk_column in cdata['columns']:
                if cdata['unique'] or (cdata['foreign_key'] and drop_fk):
                    # Drop constraint
                    self.execute(self.sql_delete_unique % {
                                'table': self.quote_name(m2m_table),
                                'name': self.quote_name(cname)})                                            
                elif cdata['index'] and not cdata['primary_key']:
                    # Drop index
                    self.execute(f'DROP INDEX {self.quote_name(cname)}')

        # --- Alter column type ---
        sql = self.sql_alter_column_type % {
                'column': self.quote_name(m2m_fk_column),
                'type': pk_new_type
            }
        self.execute(
             self.sql_alter_column % {
                'table': self.quote_name(m2m_table),
                'changes': sql
            }
        )
        self._reorg_tables()

        # --- Recreate constraints ---
        for cname, cdata in constraints.items():
            if m2m_fk_column in cdata['columns']:
                if cdata['foreign_key']:
                    # Recreate FK (construct full SQL using cdata['foreign_key'])
                    fk_table, fk_column = cdata['foreign_key']                                            
                    self.execute(
                        self.sql_create_fk % {
                            'table': self.quote_name(m2m_table),                            
                            'name': self.quote_name(cname),
                            'column': self.quote_name(m2m_fk_column),
                            'to_table': self.quote_name(fk_table),
                            'to_column': self.quote_name(fk_column),
                        }
                    )
                                                       
                if cdata['unique']:
                    # Recreate unique
                    self.execute(self.sql_create_unique % {
                        'table': self.quote_name(m2m_table),
                        'name': self.quote_name(cname),
                        'columns': ", ".join(self.quote_name(col) for col in cdata["columns"]),
                        'deferrable': self._deferrable_constraint_sql(None),
                    })
                elif cdata['index'] and not cdata['primary_key']:
                    create_index_sql = (
                        f'CREATE INDEX {self.quote_name(cname)} '
                        f'ON {self.quote_name(m2m_table)} ({", ".join(self.quote_name(col) for col in cdata["columns"])})'
                    )
                    self.execute(create_index_sql)
                    
    def alterFieldDataTypeForM2MTables(self, m2m_constraints, pk_new_type):
        #Drop all unique, index constraints before altering field data type.
        #Foreign key constraints are auto dropped when parent table is dropped.
        #Alter column type of m2m tables.
        #Recreate constraints.
        for (m2m_table, m2m_fk_column), constraints in m2m_constraints.items():
            self.alterFieldDataTypeForM2MTables_post(m2m_table, m2m_fk_column, constraints, pk_new_type, False)

    def _extract_field_names_from_constraint(self, constraint):
        """
        Extract field names referenced in a CheckConstraint's condition. 
        Returns a set of field names. 
        
        Example:  
            constraint:  CheckConstraint(condition=Q(height__gte=0), name="height_gte_0")
            returns:  {'height'}
        """
        from django.db.models import CheckConstraint, Q
        
        field_names = set()
        
        if not isinstance(constraint, CheckConstraint):
            return field_names
        
        # The condition is a Q object; traverse it to find field lookups
        def traverse_q(q_obj):
            """Recursively traverse Q object to extract field names."""
            for child in q_obj.children:
                if isinstance(child, Q):
                    # Nested Q object, recurse
                    traverse_q(child)
                elif isinstance(child, tuple) and len(child) == 2:
                    # child is (lookup_string, value)
                    # Example: ("height__gte", 0) -> extract "height"
                    lookup = child[0]
                    # Extract field name from lookup (e.g., "height__gte" -> "height")
                    field_name = lookup.split('__')[0]
                    field_names. add(field_name)
        
        traverse_q(constraint. condition)
        return field_names

    def alterFieldDataTypeByRemaking(self, model, old_field, new_field, strict):
        """
        Remake table for field rename and/or type change when DB2 requires it.
        Handles column renames (e.g., 'id' -> 'note_ptr') robustly.
        """
        import copy
        import re
    
        internal_types = ('AutoField', 'BigAutoField', 'SmallAutoField')
        needs_remake = ((old_field.primary_key or (not new_field.null and not new_field.has_default()))
                        and not (old_field.name == new_field.name and 
                                 (new_field.get_internal_type() == 'OneToOneField' or 
                                  new_field.get_internal_type() == 'ForeignKey'))
                        #and not (not new_field.primary_key and new_field.unique)
                        )
        # Only remake, if column PK is changed or when if its NOT NULL, and there's no default
        if needs_remake:
            old_table = model._meta.db_table
            tmp_table = f"{old_table}_TMP"
    
            # Save old fields            
            old_fields = [copy.deepcopy(f) for f in model._meta.local_fields]
            
            # Build final fields for new schema, reflecting rename
            new_fields = []            
            seen_columns = set()

            for f in old_fields:
                candidate = copy.deepcopy(new_field) if f.name == old_field.name else copy.deepcopy(f)
                if candidate.column in seen_columns:
                    continue
                seen_columns.add(candidate.column)
                new_fields.append(candidate)
                
            # Now use this fields list for creating columns and data mapping
            new_columns = []
            for f in new_fields:
                col_name = self.quote_name(f.column)
                
                # Check if field is part of composite PK
                is_pk_field = not f.null or f.primary_key
                if hasattr(model._meta, 'pk') and hasattr(model._meta.pk, 'field_names'):
                    is_pk_field = is_pk_field or (f.name in model._meta.pk.field_names)
                
                definition, params = self.column_sql(
                    model, f, include_default=True, include_null=True, notnull=is_pk_field, 
                    skip_force_default=True
                )
                if definition is None:
                    continue
                if hasattr(f, "db_type_suffix"):
                    col_type_suffix = f.db_type_suffix(connection=self.connection)
                    if col_type_suffix:
                        definition += f" {col_type_suffix}"
                new_columns.append(f"{col_name} {definition}")
            
            # CREATE TABLE with the right (future) columns!
            create_sql = "CREATE TABLE %s (%s)" % (self.quote_name(tmp_table), ", ".join(new_columns))
            self.execute(create_sql)
    
            # Build INSERT with old->new column mapping ===
            insert_columns = [self.quote_name(f.column) for f in new_fields]
            select_columns = []
            
            for old_f, new_f in zip(old_fields, new_fields):
                col_name = self.quote_name(old_f.column)
                # Detect type change needing a cast
                old_type = type(old_f)
                new_type = type(new_f)
            
                # TEXT/CLOB/VARCHAR to DATE/DATETIME/TIME
                if (
                    isinstance(old_f, (CharField, TextField))
                    and isinstance(new_f, (DateField, DateTimeField, TimeField))
                ):
                    # CLOB -> VARCHAR -> DATE/TIMESTAMP/TIME for DB2
                    if isinstance(new_f, DateField):
                        select_columns.append(f'CAST(CAST({col_name} AS VARCHAR(255)) AS DATE)')
                    elif isinstance(new_f, DateTimeField):
                        select_columns.append(f'CAST(CAST({col_name} AS VARCHAR(255)) AS TIMESTAMP)')
                    elif isinstance(new_f, TimeField):
                        select_columns.append(f'CAST(CAST({col_name} AS VARCHAR(255)) AS TIME)')
                        #select_columns.append(f'CAST(TRIM(CAST({col_name} AS VARCHAR(255))) AS TIME)')
                    else:
                        select_columns.append(col_name)
                        
                # TEXT/VARCHAR to INTEGER/NUMERIC types
                elif (
                    isinstance(old_f, (CharField, TextField))
                    and isinstance(new_f, (IntegerField, FloatField, DecimalField))
                ):
                    # DB2: you may want to use INTEGER, FLOAT, DECFLOAT as appropriate
                    # (Real casting may require more nuance, but this is a safe basic start)
                    if isinstance(new_f, IntegerField):
                        select_columns.append(f"CAST({col_name} AS INTEGER)")
                    elif isinstance(new_f, FloatField):
                        select_columns.append(f"CAST({col_name} AS FLOAT)")
                    elif isinstance(new_f, DecimalField):
                        select_columns.append(f"CAST({col_name} AS DECFLOAT(34))")
                    else:
                        select_columns.append(col_name)
                else:
                    select_columns.append(col_name)
            
            insert_sql = "INSERT INTO %s (%s) SELECT %s FROM %s" % (
                self.quote_name(tmp_table),
                ", ".join(insert_columns),
                ", ".join(select_columns),
                self.quote_name(old_table)
            )
            self.execute(insert_sql)

            # Constraints: gather/fix as in your system ===
            if old_field.primary_key:
                m2m_constraints = self.getM2MTablesConstraints(model)
            self.execute(f"DROP TABLE {self.quote_name(old_table)}")
            self.execute(f"RENAME TABLE {self.quote_name(tmp_table)} TO {self.quote_name(old_table)}")
    
            # Reset identity if needed
            with self.connection.cursor() as cursor:
                if new_field.get_internal_type() in internal_types:
                    pk_column = self.quote_name(new_field.column)
                    table_name = self.quote_name(model._meta.db_table)
                    cursor.execute(
                        f"SELECT COALESCE(MAX({pk_column}), 0) + 1 FROM {table_name}"
                    )
                    restart_value = cursor.fetchone()[0]
                    cursor.execute(
                        f"ALTER TABLE {table_name} ALTER COLUMN {pk_column} RESTART WITH {restart_value}"
                    )
    
            # Recreate indexes/constraints
            #for sql in self._model_indexes_sql(model):
            #    self.execute(sql)
    
            # After RENAME TABLE, recreate indexes from new_fields
            for field in new_fields:
                if field.db_index and not field.unique and not field.primary_key:
                    index_name = self._create_index_name(
                        model._meta.db_table,
                        [field.column],  # Use NEW column name
                        suffix='_idx'
                    )
                    self.execute(
                        f'CREATE INDEX {self.quote_name(index_name)} '
                        f'ON {self.quote_name(model._meta.db_table)} '
                        f'({self.quote_name(field.column)})'
                    )
                    
            # Handle referencing FKs (custom logic here...)
            if old_field.primary_key:
                pk_new_type = new_field.db_type(self.connection)
                self.alterFieldDataTypeForM2MTables(m2m_constraints, pk_new_type)
                referencing_fks = self.getAllReferencingFKs(model)
                for fk_table, fk_column in referencing_fks:
                    with self.connection.cursor() as cursor:
                        constraints = self.connection.introspection.get_constraints(cursor, fk_table)
                    self.alterFieldDataTypeForM2MTables_post(fk_table, fk_column, constraints, pk_new_type, True)
    
            self._reorg_tables()
            self.execute("COMMIT")
            return old_field, new_field
        else:    
            tmp_new_field = copy.deepcopy(new_field)
            tmp_new_field.column = truncate_name( "%s%s" % ( self.psudo_column_prefix, tmp_new_field.column ), self.connection.ops.max_name_length() )
    
            self.__model = model            
            notnull = not tmp_new_field.null
            #tmp_new_field.null = True
            p_key = tmp_new_field.primary_key
            #tmp_new_field.primary_key = False
            unique = tmp_new_field.unique
            #tmp_new_field._unique = False
            if p_key or unique:
                notnull = True
                
            #When adding a column to existing table with rows in it, NOT NULL clause cannot be enforced 
            #until default is specified. Hence include_null=False here.
            if unique and notnull:
                tmp_new_field.unique = False
            _ , skip_post = self.add_field_pre(model, tmp_new_field, unique, notnull, include_null=False)
            if skip_post:
                return
    
            new_value = self.quote_name(old_field.column)
            old_type = old_field.db_type(self.connection)
            if re.match('^N?CLOB', old_type):
                old_type = 'VARCHAR'
            if re.match('^N?VARCHAR', old_type):
                new_internal_type = new_field.get_internal_type()
                if new_internal_type == 'DateField':
                    new_value = "TO_DATE(%s, 'YYYY-MM-DD')" % new_value
                elif new_internal_type == 'DateTimeField':
                    new_value = "TO_TIMESTAMP(%s, 'YYYY-MM-DD HH24:MI:SS.FF')" % new_value
                elif new_internal_type == 'TimeField':
                    # TimeField are stored as TIMESTAMP with a 1900-01-01 date part.
                    new_value = "TO_TIMESTAMP(CONCAT('1900-01-01 ', %s), 'YYYY-MM-DD HH24:MI:SS.FF')" % new_value
    
            #Transfer data from old field to new tmp field
            self.execute("UPDATE %s set %s=%s" % (
                    self.quote_name(model._meta.db_table),
                    self.quote_name(tmp_new_field.column),
                    new_value
                )
            )
    
            if (not tmp_new_field.has_db_default() and 
                not self.skip_default(tmp_new_field) and 
                self.effective_default(tmp_new_field) is not None):
                #UPDATE "ZURBIE"."SCHEMA_AUTHOR" SET PSUDO_HEIGHT = 42 WHERE PSUDO_HEIGHT IS NULL;
                self.execute("UPDATE %s SET %s = %s WHERE %s IS NULL" % (
                        self.quote_name(model._meta.db_table),
                        self.quote_name(tmp_new_field.column),
                        self.effective_default(tmp_new_field),
                        self.quote_name(tmp_new_field.column)
                    ) 
                )
                
            self.add_field_post(model, tmp_new_field, p_key, unique, notnull)
            self.remove_field(model, old_field)
            
            return tmp_new_field, new_field

    def _remake_table_with_new_pk(self, model, create_field):
        """
        Recreates the table with a new PK, retaining all old columns but
        only the new field as PK.
        """
        import copy
        old_table = model._meta.db_table
        tmp_table = "new__%s" % old_table
    
        old_field = None
        new_field = None
        deferred_constraints = {
            'pk': {},
            'fk': {},
            'unique': {},
            'index': {},
            'check': {}}
        
        #Defer constraint check
        with self.connection.cursor() as cur:
            constraints_pre = self.connection.introspection.get_constraints(cur, old_table)
        self._defer_constraints_check(constraints_pre, deferred_constraints, old_field, new_field, model, defer_pk=True, defer_unique=True, defer_index=True, defer_check=True, defer_fk=True, rename_table=old_table)        

        # Step 1: Prepare new fields list
        fields = []
        for f in model._meta.local_fields:
            field_copy = copy.deepcopy(f)
            # Turn off primary_key flag on the old PK
            if field_copy.primary_key and create_field.primary_key:
                field_copy.primary_key = False
                field_copy.unique = False
            fields.append(field_copy)
        # Add the new PK field if not already in fields
        if not any(f.name == create_field.name for f in fields):
            fields.append(create_field)
    
        # Step 2: Build SQL for temp table
        columns_sql = []
        for f in fields:
            # Check if field is part of composite PK or is the new PK
            is_pk_field = not f.null or f.primary_key or (f.name == create_field. name)
            if hasattr(model._meta, 'pk') and hasattr(model._meta.pk, 'field_names'):
                is_pk_field = is_pk_field or (f.name in model._meta. pk.field_names)
            
            definition, params = self.column_sql(model, f, include_default=True, include_null=True, 
                                                 notnull=is_pk_field, skip_force_default=False)
            if definition is None:
                continue
            col_type_suffix = f.db_type_suffix(connection=self.connection) if hasattr(f, "db_type_suffix") else None
            if col_type_suffix:
                definition += f" {col_type_suffix}"
            columns_sql.append("%s %s" % (self.quote_name(f.column), definition))
    
        create_sql = "CREATE TABLE %s (%s)" % (self.quote_name(tmp_table), ", ".join(columns_sql))
        self.execute(create_sql)
    
        # Copy data: skip Auto PK if it's new (let DB2 generate)
        data_fields = [f for f in fields if not (f.primary_key and f.name == create_field.name)]
        insert_cols = [self.quote_name(f.column) for f in data_fields]
        select_cols = [self.quote_name(f.column) for f in data_fields]
        
        insert_sql = "INSERT INTO %s (%s) SELECT %s FROM %s" % (
            self.quote_name(tmp_table),
            ", ".join(insert_cols),
            ", ".join(select_cols),
            self.quote_name(old_table)
        )
        self.execute(insert_sql)
            
        # Step 4: Drop old table and rename
        self.execute("DROP TABLE %s" % self.quote_name(old_table))
        self.execute("RENAME TABLE %s TO %s" % (self.quote_name(tmp_table), self.quote_name(old_table)))
        
        #restore constraint checks
        deferred_constraints = self.get_missing_constraints(model, constraints_pre, deferred_constraints)
        self._restore_constraints_check(deferred_constraints, old_field, new_field, model, rename_table=old_table, old_table=old_table)
                    
        self._reorg_tables()
        self.execute("COMMIT")
        
    def add_field(self, model, field):
        self.__model = model
        notnull = not field.null
        #field.null = True
        p_key = field.primary_key
        #field.primary_key = False
        unique = field.unique
        #field._unique = False
        if p_key or unique:
            notnull = True
            
        #need to handle if new field added is primary key and if there is already a primary key in table
        # Does the model already have a PK?
        existing_pks = [f for f in model._meta.local_fields if f.primary_key]
        if field.primary_key and existing_pks:
            # There is already a PK, and the new field also wants to be PK: must remake the table, etc.
            self._remake_table_with_new_pk(model, field)
            return

        # If UNIQUE & NOT NULL (O2O) on existing table: do multi-phase add for DB2.
        if unique and notnull:
            # 1. Add as NULLABLE, no unique constraint
            field_copy = copy.deepcopy(field)
            field_copy.unique = False
            _, skip_post = self.add_field_pre(model, field_copy, unique, notnull, include_null=False)
            if skip_post:
                return
            
            self.add_field_post(model, field_copy, p_key, field_copy.unique, notnull)
            return
    
        _ , skip_post = self.add_field_pre(model, field, unique, notnull, include_null=False)
        if skip_post:
            return
        self.add_field_post(model, field, p_key, unique, notnull)

    def alter_db_table(self, model, old_db_table, new_db_table):
        old_field = None
        new_field = None
        suffix = '_fk_%(to_table)s_%(to_column)s'
        deferred_constraints = {
            'pk': {},
            'fk': {},
            'unique': {},
            'index': {},
            'check': {}}

        if (old_db_table == new_db_table or
            (self.connection.features.ignores_table_name_case and
                old_db_table.lower() == new_db_table.lower())):
            return

        #Defer constraint check
        with self.connection.cursor() as cur:
            constraints_pre = self.connection.introspection.get_constraints(cur, old_db_table)
        self._defer_constraints_check(constraints_pre, deferred_constraints, old_field, new_field, model, defer_pk=True, defer_unique=True, defer_index=True, defer_check=True, defer_fk=True, rename_table=old_db_table)        

        self.execute(self.sql_rename_table % {
            "old_table": self.quote_name(old_db_table),
            "new_table": self.quote_name(new_db_table),
        })

        #restore constraint checks
        deferred_constraints = self.get_missing_constraints(model, constraints_pre, deferred_constraints)
        self._restore_constraints_check(deferred_constraints, old_field, new_field, model, rename_table=new_db_table, old_table=old_db_table)

        #Rebuild incoming FK constraints
        for inc_rel in model._meta.related_objects:
            if not isinstance(inc_rel, ManyToManyRel):
                #If self-referential ignore, already restored constraints above.
                if inc_rel.field.opts.db_table != new_db_table and old_db_table == inc_rel.field.related_model._meta.db_table:
                    self.execute(
                        self.sql_create_fk % {
                            'table': self.quote_name(inc_rel.field.opts.db_table),
                            'name': self._create_index_name(inc_rel.field.opts.db_table, [inc_rel.field.column], suffix="_fk"),
                            'column': self.quote_name(inc_rel.field.column),
                            'to_table': self.quote_name(new_db_table),
                            'to_column': self.quote_name(inc_rel.field.target_field.column),
                        }
                    )

        # Rename all references to the old table name.
        for sql in self.deferred_sql:
            if isinstance(sql, Statement):
                sql.rename_table_references(old_db_table, new_db_table)
        
    def _alter_many_to_many(self, model, old_field, new_field, strict):
        deferred_constraints = {
                          'pk': {},
                          'unique': {},
                          'index': {},
                          'check': {}}

        if( djangoVersion[0:2] < ( 1, 9 ) ):
            if ((old_field.rel is not None and hasattr(old_field.rel,'through')) and 
               (new_field.rel is not None and hasattr(new_field.rel,'through'))):
                old_field_rel_through = old_field.rel.through
                rel_old_field = old_field.rel.through._meta.get_field(old_field.m2m_reverse_field_name())[0]
                rel_new_field = new_field.rel.through._meta.get_field(new_field.m2m_reverse_field_name())[0]
            else:
                rel_old_field = None
                rel_new_field = None
        else:
            if((old_field.remote_field is not None and hasattr(old_field.remote_field,'through')) and 
                (new_field.remote_field is not None and hasattr(new_field.remote_field,'through'))):
                old_field_rel_through = old_field.remote_field.through
                rel_old_field = old_field.remote_field.through._meta.get_field(old_field.m2m_reverse_field_name())
                rel_new_field = new_field.remote_field.through._meta.get_field(new_field.m2m_reverse_field_name())
            else:
                rel_old_field = None
                rel_new_field = None

        if((rel_old_field is not None) and (rel_new_field is not None)):
            """Alter M2Ms to repoint their to= endpoints."""
            # Rename the through table
            if old_field.remote_field.through._meta.db_table != new_field.remote_field.through._meta.db_table:
                self.alter_db_table(model, old_field.remote_field.through._meta.db_table,
                                    new_field.remote_field.through._meta.db_table)
            # Repoint the FK to the other side
            self.alter_field(
                new_field.remote_field.through,
                # We need the field that points to the target model, so we can tell alter_field to change it -
                # this is m2m_reverse_field_name() (as opposed to m2m_field_name, which points to our model)
                old_field.remote_field.through._meta.get_field(old_field.m2m_reverse_field_name()),
                new_field.remote_field.through._meta.get_field(new_field.m2m_reverse_field_name()),
            )
            self.alter_field(
                new_field.remote_field.through,
                # for self-referential models we need to alter field from the other end too
                old_field.remote_field.through._meta.get_field(old_field.m2m_field_name()),
                new_field.remote_field.through._meta.get_field(new_field.m2m_field_name()),
            )
       
    def _reorg_tables(self):
        """
        Reorganize DB2 tables that are in reorg pending state.
        
        Automatically runs RUNSTATS after REORG to optimize query performance.
        
        Handles:
        - ALLOW WRITE ACCESS to minimize downtime
        - Automatic RUNSTATS for performance optimization
        - User feedback via logging and stdout
        - Error handling with fallback
        """
        import logging
        import time
        
        logger = logging.getLogger('django.db.backends')
        
        # Check for tables in reorg pending state
        checkReorgSQL = (
            "SELECT TABSCHEMA, TABNAME FROM SYSIBMADM.ADMINTABINFO "
            "WHERE REORG_PENDING = 'Y'"
        )
        res = []
        
        with self.connection.cursor() as cursor:
            cursor.execute(checkReorgSQL)
            res = cursor.fetchall()
        
        if not res:
            logger.debug("No tables in reorg pending state")
            return
        
        # User notification
        logger.warning(
            "DB2 OPTIMIZATION: Found %d table(s) requiring REORG. "
            "Starting reorganization and statistics update (this may take several minutes for large tables)..." % len(res)
        )
        print(
            "\n" + "="*80 + "\n"
            "DB2 TABLE OPTIMIZATION IN PROGRESS\n"
            "="*80 + "\n"
            "Found %d table(s) requiring reorganization.\n"
            "This process includes:\n"
            "  1. REORG - Reorganize physical table storage\n"
            "  2. RUNSTATS - Update statistics for query optimization\n\n"
            "This may take several minutes for large tables...\n" % len(res)
        )
        
        reorg_stats = {
            'reorg_successful': 0,
            'reorg_failed': 0,
            'runstats_successful': 0,
            'runstats_failed': 0,
            'tables': []
        }
        
        for sName, tName in res:
            full_table_name = '"%s"."%s"' % (sName, tName)
            table_info = {
                'name': full_table_name,
                'reorg_status': 'pending',
                'runstats_status': 'pending',
                'reorg_error': None,
                'runstats_error': None,
                'reorg_time': 0,
                'runstats_time': 0,
            }
            
            # STEP 1: REORG TABLE
            try:
                start_time = time.time()
                
                # Try REORG with ALLOW WRITE ACCESS first (minimizes downtime)
                reorgSQL_with_write_access = (
                    "CALL SYSPROC.ADMIN_CMD('REORG TABLE %s ALLOW WRITE ACCESS')" % full_table_name
                )
                
                logger.info("REORG [1/2] starting for %s (with ALLOW WRITE ACCESS)..." % full_table_name)
                print("  [1/2] REORG: %s" % full_table_name)
                
                with self.connection.cursor() as cursor:
                    cursor.execute(reorgSQL_with_write_access)
                
                reorg_time = time.time() - start_time
                table_info['reorg_time'] = round(reorg_time, 2)
                table_info['reorg_status'] = 'success_with_write_access'
                
                logger.info(
                    "REORG [1/2] completed for %s "
                    "(with ALLOW WRITE ACCESS) in %.2f seconds" % (full_table_name, reorg_time)
                )
                reorg_stats['reorg_successful'] += 1
                
            except Exception as e:
                logger.warning(
                    "REORG with ALLOW WRITE ACCESS failed for %s: %s\n"
                    "   Retrying without ALLOW WRITE ACCESS (table may be locked during REORG)..." % (full_table_name, str(e))
                )
                
                try:
                    start_time = time.time()
                    
                    # Fallback: Try REORG without ALLOW WRITE ACCESS
                    reorgSQL_without_access = (
                        "CALL SYSPROC.ADMIN_CMD('REORG TABLE %s')" % full_table_name
                    )
                    
                    print("  [1/2] REORG (retry): %s" % full_table_name)
                    
                    with self.connection.cursor() as cursor:
                        cursor.execute(reorgSQL_without_access)
                    
                    reorg_time = time.time() - start_time
                    table_info['reorg_time'] = round(reorg_time, 2)
                    table_info['reorg_status'] = 'success_without_write_access'
                    
                    logger.info(
                        "REORG [1/2] completed (fallback) for %s in %.2f seconds" % (full_table_name, reorg_time)
                    )
                    reorg_stats['reorg_successful'] += 1
                    
                except Exception as e2:
                    logger.error("REORG [1/2] FAILED for %s: %s" % (full_table_name, str(e2)))
                    print("  [1/2] REORG FAILED: %s\n        Error: %s" % (full_table_name, str(e2)))
                    table_info['reorg_status'] = 'failed'
                    table_info['reorg_error'] = str(e2)
                    reorg_stats['reorg_failed'] += 1
                    reorg_stats['tables'].append(table_info)
                    raise
            
            # STEP 2: RUNSTATS (only if REORG succeeded)
            try:
                # Check if RUNSTATS is needed
                checkRunstatsSQL = (
                    "SELECT STATS_RUN FROM SYSIBMADM.ADMINTABINFO "
                    "WHERE TABSCHEMA = ? AND TABNAME = ?"
                )
                
                needs_runstats = False
                with self.connection.cursor() as cursor:
                    cursor.execute(checkRunstatsSQL, [sName, tName])
                    result = cursor.fetchone()
                    if result and result[0] == 'N':
                        needs_runstats = True
                
                if needs_runstats:
                    start_time = time.time()
                    
                    logger.info("RUNSTATS [2/2] starting for %s..." % full_table_name)
                    print("  [2/2] RUNSTATS: %s" % full_table_name)
                    
                    # Run RUNSTATS with full options
                    # WITH DISTRIBUTION AND DETAILED INDEXES ALL provides comprehensive statistics
                    runstatsSQL = (
                        "CALL SYSPROC.ADMIN_CMD("
                        "'RUNSTATS ON TABLE %s "
                        "WITH DISTRIBUTION AND DETAILED INDEXES ALL')" % full_table_name
                    )
                    
                    with self.connection.cursor() as cursor:
                        cursor.execute(runstatsSQL)
                    
                    runstats_time = time.time() - start_time
                    table_info['runstats_time'] = round(runstats_time, 2)
                    table_info['runstats_status'] = 'success'
                    
                    logger.info(
                        "RUNSTATS [2/2] completed for %s in %.2f seconds" % (full_table_name, runstats_time)
                    )
                    print("        RUNSTATS completed in %.2f seconds" % runstats_time)
                    reorg_stats['runstats_successful'] += 1
                else:
                    logger.info(
                        "RUNSTATS [2/2] not needed for %s "
                        "(statistics are current)" % full_table_name
                    )
                    print("  [2/2] RUNSTATS: Not needed (statistics current)")
                    table_info['runstats_status'] = 'skipped_current'
                
            except Exception as e:
                # Don't fail migration if RUNSTATS fails - it's optimization
                logger.warning(
                    "RUNSTATS [2/2] skipped for %s: %s\n"
                    "   This is not critical. Queries will still work, but may not be optimally fast.\n"
                    "   Consider running RUNSTATS manually later." % (full_table_name, str(e))
                )
                print("  [2/2] RUNSTATS skipped: %s" % str(e))
                table_info['runstats_status'] = 'failed'
                table_info['runstats_error'] = str(e)
                reorg_stats['runstats_failed'] += 1
            
            reorg_stats['tables'].append(table_info)
        
        # Summary report
        total_time = sum(t['reorg_time'] + t['runstats_time'] for t in reorg_stats['tables'])
        
        print(
            "\n" + "="*80 + "\n"
            "DB2 OPTIMIZATION SUMMARY\n"
            "="*80 + "\n"
            "REORG Results:\n"
            "  Successful: %d\n"
            "  Failed: %d\n\n"
            "RUNSTATS Results:\n"
            "  Completed: %d\n"
            "  Skipped or Failed: %d\n\n"
            "Total Time: %.2f seconds\n"
            "="*80 + "\n" % (
                reorg_stats['reorg_successful'],
                reorg_stats['reorg_failed'],
                reorg_stats['runstats_successful'],
                reorg_stats['runstats_failed'],
                total_time
            )
        )
        
        # Detailed table breakdown
        if reorg_stats['tables']:
            print("Detailed Results:")
            for table in reorg_stats['tables']:
                print("\n  Table: %s" % table['name'])
                print("    REORG: %s (%.2f seconds)" % (table['reorg_status'], table['reorg_time']))
                if table['reorg_error']:
                    print("      Error: %s" % table['reorg_error'])
                print("    RUNSTATS: %s (%.2f seconds)" % (table['runstats_status'], table['runstats_time']))
                if table['runstats_error']:
                    print("      Error: %s" % table['runstats_error'])
            print()
        
        # Log summary
        if reorg_stats['reorg_successful'] > 0:
            logger.info(
                "DB2 REORG: %d table(s) reorganized successfully" % reorg_stats['reorg_successful']
            )
        
        if reorg_stats['runstats_successful'] > 0:
            logger.info(
                "DB2 RUNSTATS: %d table(s) statistics updated successfully" % reorg_stats['runstats_successful']
            )
        
        if reorg_stats['reorg_failed'] > 0:
            logger.error(
                "DB2 REORG: %d table(s) failed to reorganize" % reorg_stats['reorg_failed']
            )
        
        if reorg_stats['runstats_failed'] > 0:
            logger.warning(
                "DB2 RUNSTATS: %d table(s) failed to run RUNSTATS "
                "(queries will still work but may not be optimally fast)" % reorg_stats['runstats_failed']
            )
        
    def _defer_constraints_check(self, constraints, deferred_constraints, old_field, new_field, model, defer_pk=False, defer_unique=False, defer_index=False, defer_check=False, defer_fk=False, rename_table=None):
        for constr_name, constr_dict in list(constraints.items()):
            old_col_name = ''
            if old_field is not None:
                if old_field.column.count('""') > 0:
                    old_col_name = old_field.column.replace('""', '"')
                else:
                    old_col_name = old_field.column
            if constr_name is not None and constr_name.find('"', 1, len(constr_name)-1) > 0:
                constr_name = constr_name.replace('"', '""')
            columns_list = []
            if len(constr_dict['columns']) > 0:                
                for cols in constr_dict['columns']:
                    if cols.find('"', 1, len(cols)-1) > 0:
                        columns_list.append(cols.replace('"', '""'))
                    else:
                        columns_list.append(cols)
            if (old_field is not None and old_col_name in constr_dict['columns']) or rename_table is not None:
                if defer_check and constr_dict['check'] is True:
                    self.execute(self.sql_delete_check % {
                                        'table': self.quote_name(model._meta.db_table if rename_table is None else rename_table),
                                        'name': self.quote_name(constr_name)
                                })
                    deferred_constraints['check'][constr_name] = columns_list
                elif defer_index and constr_dict['index'] is True and 'type' in constr_dict.keys():
                    try:
                        self.execute(self.sql_delete_index % {
                                            'table': self.quote_name(model._meta.db_table if rename_table is None else rename_table),
                                            'name': self.quote_name(constr_name)
                                    })
                        deferred_constraints['index'][constr_name] = columns_list
                    except:
                        pass
                elif defer_fk and constr_dict['foreign_key'] is not None:
                    try:
                        self.execute(self.sql_delete_pk % {
                                            'table': self.quote_name(model._meta.db_table if rename_table is None else rename_table),
                                            'name': self.quote_name(constr_name)})
                        deferred_constraints['fk'][constr_name] = constr_dict
                    except:
                        continue
                elif defer_unique and constr_dict['unique'] is True:
                    try:
                        self.execute(self.sql_delete_unique % {
                                            'table': self.quote_name(model._meta.db_table if rename_table is None else rename_table),
                                            'name': self.quote_name(constr_name)})
                        deferred_constraints['unique'][constr_name] = columns_list
                    except:
                        continue
                elif defer_pk and constr_dict['primary_key'] is True and 'type' not in constr_dict.keys():
                    self.execute(self.sql_drop_pk % {
                                        'table': self.quote_name(model._meta.db_table if rename_table is None else rename_table),
                                        })                    
                    deferred_constraints['pk'][constr_name] = columns_list
            
        return deferred_constraints
    
    def _normalize_constraint_name(self, name, old_field=None, new_field=None, old_table=None, new_table=None):
        """
        Normalize a constraint/index name after table or column renames.
        Only does simple exact replacements for table and column tokens.
        Returns an upper-cased name (consistent with the rest of your code).
        """
        if old_table and new_table and old_table in name:
            name = name.replace(old_table, new_table)
        if old_field is not None and new_field is not None and old_field.column in name:
            name = name.replace(old_field.column, new_field.column)
        return name.upper()
    
    
    def _normalize_columns(self, columns, old_field=None, new_field=None, rename_table=None):
        """
        Normalize a list of column names after a field rename.
        Uses exact equality match for each column (no substring replace).
        If rename_table is provided, we assume columns are already correct for that table
        and do not modify them here.
        """
        if rename_table or old_field is None or new_field is None:
            return list(columns)
        # exact-match replacement only
        return [ (new_field.column if column == old_field.column else column) for column in columns ]


    def _restore_constraints_check(self, deferred_constraints, old_field, new_field, model, rename_table=None, old_table=None):
        def create_fk_name(*args, **kwargs):
            return self.quote_name(self._create_index_name(*args, **kwargs))
    
        self.__model = model
        table = model._meta.db_table if rename_table is None else rename_table
    
        # Primary keys
        for pk_name, columns in list(deferred_constraints.get('pk', {}).items()):
            pk_name = self._normalize_constraint_name(pk_name, old_field, new_field, old_table, rename_table)
            cols = self._normalize_columns(columns, old_field, new_field, rename_table)
            self.execute(self.sql_create_pk % {
                'table': self.quote_name(table),
                'name': self.quote_name(pk_name),
                'columns': ', '.join(cols).upper(),
            })
    
        # Foreign keys
        if 'fk' in deferred_constraints:
            suffix = '_fk_%(to_table)s_%(to_column)s'
            for fk_name, constr_dict in list(deferred_constraints['fk'].items()):
                cols = self._normalize_columns(constr_dict['columns'], old_field, new_field, rename_table)
                to_table = rename_table.upper() if (old_table and old_table == constr_dict['foreign_key'][0]) else constr_dict['foreign_key'][0].upper()
                to_column = self.quote_name(new_field.column) if new_field is not None else self.quote_name(constr_dict['foreign_key'][1])
                from_table = table
                column_sql = ', '.join(cols).upper()
                fk_name_obj = ForeignKeyName(
                    from_table.upper(),
                    [column_sql],
                    to_table,
                    to_column,
                    suffix,
                    create_fk_name,
                )
                self.execute(self.sql_create_fk % {
                    'table': self.quote_name(from_table),
                    'name': fk_name_obj,
                    'column': column_sql,
                    'to_table': to_table,
                    'to_column': to_column,
                })
    
        # Indexes
        for index_name, columns in list(deferred_constraints.get('index', {}).items()):
            try:
                index_name = self._normalize_constraint_name(index_name, old_field, new_field, old_table, rename_table)
                cols = self._normalize_columns(columns, old_field, new_field, rename_table)
                self.execute(self.sql_create_index % {
                    'table': self.quote_name(table),
                    'name': index_name,
                    'columns': ', '.join(cols).upper(),
                    'extra': "",
                    'condition': "",
                    'include': "",
                })
            except Exception:
                continue
    
        # Unique constraints
        for constr_name, columns in list(deferred_constraints.get('unique', {}).items()):
            constr_name = self._normalize_constraint_name(constr_name, old_field, new_field, old_table, rename_table)
            cols = self._normalize_columns(columns, old_field, new_field, rename_table)
            self.execute(self.sql_create_unique % {
                'table': self.quote_name(table),
                'name': self.quote_name(constr_name),
                'columns': ', '.join(cols).upper(),
                'deferrable': self._deferrable_constraint_sql(None),
            })

    def _constraint_names(self, model, column_names=None, unique=None,
                          primary_key=None, index=None, foreign_key=None,
                          check=None, type_=None, exclude=None):
        """Return all constraint names matching the columns and conditions."""

        with self.connection.cursor() as cursor:
            constraints = self.connection.introspection.get_constraints(cursor, model._meta.db_table)
        result = []
        for name, infodict in constraints.items():
            if column_names is None or column_names == infodict['columns']:
                if unique is not None and infodict['unique'] != unique:
                    continue
                if primary_key is not None and (infodict['primary_key'] != primary_key or 'type' in infodict.keys()):
                    continue
                if index is not None and infodict['index'] != index:
                    continue
                if check is not None and infodict['check'] != check:
                    continue
                if foreign_key is not None and not infodict['foreign_key']:
                    continue
                if type_ is not None and infodict['type'] != type_:
                    continue
                if not exclude or name not in exclude:
                    result.append(name)
        return result

    def _index_condition_sql(self, condition):        
        return ''